{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8241658,"sourceType":"datasetVersion","datasetId":4863559}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# =====================================================================\n# BLOCK 1: SETUP AND IMPORTS\n# =====================================================================\n# This block sets up the environment, imports libraries, and configures GPU\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress TensorFlow warnings\nos.environ['PYTHONHASHSEED'] = '42'  # For reproducibility\n\nimport numpy as np\nnp.random.seed(42)  # Set numpy random seed\n\nimport tensorflow as tf\ntf.random.set_seed(42)  # Set TensorFlow random seed\n\n# Configure GPU memory growth to avoid OOM errors\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        print(\"‚úÖ GPU memory growth enabled\")\n    except RuntimeError as e:\n        print(f\"‚ö†Ô∏è GPU setup error: {e}\")\n\n# Import essential libraries\nimport matplotlib.pyplot as plt\nimport time\nimport json\nimport pickle\nimport psutil\nimport humanize\nfrom sklearn.model_selection import train_test_split\nimport queue\nimport threading\nfrom copy import deepcopy\nfrom sklearn.cluster import KMeans\nfrom scipy import ndimage\n\nprint(\"=\"*70)\nprint(\"üöÄ LANE DETECTION WITH PBFT - COMPLETE WORKING VERSION\")\nprint(\"=\"*70)\nprint(f\"TensorFlow Version: {tf.__version__}\")\nprint(f\"NumPy Version: {np.__version__}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T09:06:20.726728Z","iopub.execute_input":"2026-01-30T09:06:20.727702Z","iopub.status.idle":"2026-01-30T09:06:20.750247Z","shell.execute_reply.started":"2026-01-30T09:06:20.727671Z","shell.execute_reply":"2026-01-30T09:06:20.749531Z"}},"outputs":[{"name":"stdout","text":"‚úÖ GPU memory growth enabled\n======================================================================\nüöÄ LANE DETECTION WITH PBFT - COMPLETE WORKING VERSION\n======================================================================\nTensorFlow Version: 2.19.0\nNumPy Version: 2.0.2\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# =====================================================================\n# BLOCK 2: LOAD AND PREPARE FULL CULANE DATASET\n# =====================================================================\nprint(\"\\nüìÇ LOADING COMPLETE CULANE DATASET\")\n\nBASE_PATH = \"/kaggle/input/culane-preprocessed/temp\"\nIMAGE_FOLDER = os.path.join(BASE_PATH, \"frames\")\nMASK_FOLDER = os.path.join(BASE_PATH, \"masks\")\n\nprint(f\"üìÅ Dataset paths:\")\nprint(f\"  Images: {IMAGE_FOLDER}\")\nprint(f\"  Masks:  {MASK_FOLDER}\")\n\nif not os.path.exists(IMAGE_FOLDER):\n    print(f\"‚ùå ERROR: Image folder not found: {IMAGE_FOLDER}\")\n    exit()\nif not os.path.exists(MASK_FOLDER):\n    print(f\"‚ùå ERROR: Mask folder not found: {MASK_FOLDER}\")\n    exit()\n\nall_image_files = sorted(os.listdir(IMAGE_FOLDER))\nall_mask_files = sorted(os.listdir(MASK_FOLDER))\n\nprint(f\"\\nüìä Found:\")\nprint(f\"  Images: {len(all_image_files):,}\")\nprint(f\"  Masks:  {len(all_mask_files):,}\")\n\nsample_img = os.path.join(IMAGE_FOLDER, all_image_files[0])\nsample_size_kb = os.path.getsize(sample_img) / 1024\nprint(f\"\\nüìè Sample image size: {sample_size_kb:.1f} KB\")\n\nprint(\"\\nüîç Matching ALL image-mask pairs...\")\n\nimage_map = {os.path.splitext(f)[0]: f for f in all_image_files}\nmask_map = {os.path.splitext(f)[0]: f for f in all_mask_files}\n\ncommon_keys = sorted(set(image_map.keys()) & set(mask_map.keys()))\nprint(f\"‚úÖ Matched ALL {len(common_keys):,} image-mask pairs\")\n\nimages = [image_map[k] for k in common_keys]\nmasks = [mask_map[k] for k in common_keys]\n\nprint(f\"\\nüì¶ Using COMPLETE dataset: {len(images):,} image-mask pairs\")\n\nIMG_SIZE = (224, 224)\nprint(f\"\\nüìê Image size set to: {IMG_SIZE}\")\n\nprint(f\"\\nüéØ Splitting dataset...\")\n\ntrain_images, temp_images, train_masks, temp_masks = train_test_split(\n    images, masks, test_size=0.3, random_state=42\n)\n\nval_images, test_images, val_masks, test_masks = train_test_split(\n    temp_images, temp_masks, test_size=0.333, random_state=42\n)\n\nprint(f\"\\nüìä Final dataset split:\")\nprint(f\"  Training:   {len(train_images):,} images ({len(train_images)/len(images)*100:.1f}%)\")\nprint(f\"  Validation: {len(val_images):,} images ({len(val_images)/len(images)*100:.1f}%)\")\nprint(f\"  Testing:    {len(test_images):,} images ({len(test_images)/len(images)*100:.1f}%)\")\nprint(f\"  Total:      {len(images):,} images\")\n\nprint(\"\\n‚úÖ COMPLETE dataset loaded successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T09:06:20.751670Z","iopub.execute_input":"2026-01-30T09:06:20.751982Z","iopub.status.idle":"2026-01-30T09:06:22.321906Z","shell.execute_reply.started":"2026-01-30T09:06:20.751959Z","shell.execute_reply":"2026-01-30T09:06:22.321153Z"}},"outputs":[{"name":"stdout","text":"\nüìÇ LOADING COMPLETE CULANE DATASET\nüìÅ Dataset paths:\n  Images: /kaggle/input/culane-preprocessed/temp/frames\n  Masks:  /kaggle/input/culane-preprocessed/temp/masks\n\nüìä Found:\n  Images: 120,000\n  Masks:  120,000\n\nüìè Sample image size: 38.8 KB\n\nüîç Matching ALL image-mask pairs...\n‚úÖ Matched ALL 120,000 image-mask pairs\n\nüì¶ Using COMPLETE dataset: 120,000 image-mask pairs\n\nüìê Image size set to: (224, 224)\n\nüéØ Splitting dataset...\n\nüìä Final dataset split:\n  Training:   84,000 images (70.0%)\n  Validation: 24,012 images (20.0%)\n  Testing:    11,988 images (10.0%)\n  Total:      120,000 images\n\n‚úÖ COMPLETE dataset loaded successfully!\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# =====================================================================\n# BLOCK 3: CREATE DATA PIPELINE\n# =====================================================================\n# This block creates TensorFlow data pipelines\nprint(\"\\nüöÄ CREATING DATA PIPELINE\")\n\n# Settings\nBATCH_SIZE = 16  # Adjusted for 1GB dataset\nprint(f\"üì¶ Batch size: {BATCH_SIZE}\")\n\ndef load_image_mask(img_path, mask_path):\n    \"\"\"Load and preprocess image-mask pair\"\"\"\n    # Image\n    img = tf.io.read_file(img_path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    img = tf.image.resize(img, IMG_SIZE)\n    img = tf.cast(img, tf.float32) / 255.0\n\n    # Mask\n    mask = tf.io.read_file(mask_path)\n    mask = tf.image.decode_png(mask, channels=1)\n    mask = tf.image.resize(mask, IMG_SIZE)\n    mask = tf.cast(mask > 127, tf.float32)\n\n    return img, mask\n\ndef make_dataset(image_list, mask_list, batch_size=16, shuffle=True):\n    \"\"\"Create TensorFlow dataset\"\"\"\n    img_paths = [os.path.join(IMAGE_FOLDER, f) for f in image_list]\n    mask_paths = [os.path.join(MASK_FOLDER, f) for f in mask_list]\n\n    ds = tf.data.Dataset.from_tensor_slices((img_paths, mask_paths))\n    ds = ds.map(\n        lambda x, y: load_image_mask(x, y),\n        num_parallel_calls=tf.data.AUTOTUNE\n    )\n\n    if shuffle:\n        ds = ds.shuffle(1000)\n\n    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n    return ds\n\n# Create datasets\ntrain_dataset = make_dataset(train_images, train_masks, BATCH_SIZE, True)\nval_dataset = make_dataset(val_images, val_masks, BATCH_SIZE, False)\ntest_dataset = make_dataset(test_images, test_masks, BATCH_SIZE, False)\n\n# Test the pipeline\nfor images, masks in train_dataset.take(1):\n    print(f\"‚úÖ Pipeline test: Images {images.shape}, Masks {masks.shape}\")\n    print(f\"   Image range: [{tf.reduce_min(images):.3f}, {tf.reduce_max(images):.3f}]\")\n    print(f\"   Mask range:  [{tf.reduce_min(masks):.3f}, {tf.reduce_max(masks):.3f}]\")\n\nprint(\"üéØ Data pipeline created successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T09:06:22.322963Z","iopub.execute_input":"2026-01-30T09:06:22.323330Z","iopub.status.idle":"2026-01-30T09:06:26.732938Z","shell.execute_reply.started":"2026-01-30T09:06:22.323306Z","shell.execute_reply":"2026-01-30T09:06:26.732274Z"}},"outputs":[{"name":"stdout","text":"\nüöÄ CREATING DATA PIPELINE\nüì¶ Batch size: 16\n‚úÖ Pipeline test: Images (16, 224, 224, 3), Masks (16, 224, 224, 1)\n   Image range: [0.000, 1.000]\n   Mask range:  [0.000, 1.000]\nüéØ Data pipeline created successfully!\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# =====================================================================\n# BLOCK 4: BUILD VGG16 U-NET MODEL\n# =====================================================================\n# This block creates the lane detection model\nprint(\"\\nüèóÔ∏è BUILDING VGG16 U-NET MODEL\")\n\n# Clear any previous models\ntf.keras.backend.clear_session()\n\ndef VGG16_UNet(input_shape=(224, 224, 3)):\n    \"\"\"Create VGG16-based U-Net model\"\"\"\n    base = tf.keras.applications.VGG16(weights=\"imagenet\", include_top=False, input_shape=input_shape)\n\n    # Skip connections\n    s1 = base.get_layer(\"block1_conv2\").output\n    s2 = base.get_layer(\"block2_conv2\").output\n    s3 = base.get_layer(\"block3_conv3\").output\n    s4 = base.get_layer(\"block4_conv3\").output\n    b  = base.get_layer(\"block5_conv3\").output\n\n    # Decoder\n    d1 = tf.keras.layers.Concatenate()([tf.keras.layers.UpSampling2D()(b), s4])\n    d1 = tf.keras.layers.Conv2D(512, 3, padding=\"same\", activation=\"relu\")(d1)\n\n    d2 = tf.keras.layers.Concatenate()([tf.keras.layers.UpSampling2D()(d1), s3])\n    d2 = tf.keras.layers.Conv2D(256, 3, padding=\"same\", activation=\"relu\")(d2)\n\n    d3 = tf.keras.layers.Concatenate()([tf.keras.layers.UpSampling2D()(d2), s2])\n    d3 = tf.keras.layers.Conv2D(128, 3, padding=\"same\", activation=\"relu\")(d3)\n\n    d4 = tf.keras.layers.Concatenate()([tf.keras.layers.UpSampling2D()(d3), s1])\n    d4 = tf.keras.layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\")(d4)\n\n    # Output\n    outputs = tf.keras.layers.Conv2D(1, 1, activation=\"sigmoid\")(d4)\n\n    return tf.keras.Model(inputs=base.input, outputs=outputs)\n\n# Create the model\nmodel = VGG16_UNet(input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n\n# Display model info\nprint(f\"\\nüìä MODEL SUMMARY:\")\nprint(f\"Input shape:  {model.input_shape}\")\nprint(f\"Output shape: {model.output_shape}\")\nprint(f\"Parameters:   {model.count_params():,}\")\nprint(f\"Model memory: {(model.count_params() * 4) / (1024**2):.2f} MB\")\n\nprint(\"‚úÖ Model built successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T09:06:26.734648Z","iopub.execute_input":"2026-01-30T09:06:26.734869Z","iopub.status.idle":"2026-01-30T09:06:27.342039Z","shell.execute_reply.started":"2026-01-30T09:06:26.734848Z","shell.execute_reply":"2026-01-30T09:06:27.341409Z"}},"outputs":[{"name":"stdout","text":"\nüèóÔ∏è BUILDING VGG16 U-NET MODEL\n\nüìä MODEL SUMMARY:\nInput shape:  (None, 224, 224, 3)\nOutput shape: (None, 224, 224, 1)\nParameters:   21,756,737\nModel memory: 83.00 MB\n‚úÖ Model built successfully!\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# =====================================================================\n# BLOCK 5: DEFINE LOSS FUNCTIONS AND METRICS\n# =====================================================================\n# This block defines custom loss functions for lane detection\nprint(\"\\nüìä DEFINING LOSS FUNCTIONS AND METRICS\")\n\ndef dice_coef(y_true, y_pred):\n    \"\"\"Dice coefficient metric\"\"\"\n    y_true = tf.keras.backend.flatten(y_true)\n    y_pred = tf.keras.backend.flatten(y_pred)\n    intersection = tf.keras.backend.sum(y_true * y_pred)\n    return (2. * intersection + 1) / (tf.keras.backend.sum(y_true) + tf.keras.backend.sum(y_pred) + 1)\n\ndef dice_loss(y_true, y_pred):\n    \"\"\"Dice loss\"\"\"\n    return 1 - dice_coef(y_true, y_pred)\n\ndef bce_dice_loss(y_true, y_pred):\n    \"\"\"Combined Binary Cross-Entropy + Dice loss\"\"\"\n    bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n    dice = dice_loss(y_true, y_pred)\n    return bce + dice\n\n# Also define for PBFT compatibility\ndice_coefficient = dice_coef  # Alias for PBFT\n\nprint(\"‚úÖ Loss functions defined: Dice Coefficient, BCE+Dice Loss\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T09:06:27.342968Z","iopub.execute_input":"2026-01-30T09:06:27.343248Z","iopub.status.idle":"2026-01-30T09:06:27.349200Z","shell.execute_reply.started":"2026-01-30T09:06:27.343218Z","shell.execute_reply":"2026-01-30T09:06:27.348450Z"}},"outputs":[{"name":"stdout","text":"\nüìä DEFINING LOSS FUNCTIONS AND METRICS\n‚úÖ Loss functions defined: Dice Coefficient, BCE+Dice Loss\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# =====================================================================\n# BLOCK 6: COMPILE AND TRAIN THE MODEL\n# =====================================================================\n# This block compiles and trains the model\nprint(\"\\nüöÄ STARTING MODEL TRAINING\")\n\n# Compile model\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n    loss=bce_dice_loss,\n    metrics=[dice_coef, 'binary_accuracy']\n)\nprint(\"‚úÖ Model compiled with Adam optimizer (lr=1e-4)\")\n\n# Setup training callbacks\ncallbacks = [\n    # Early stopping\n    tf.keras.callbacks.EarlyStopping(\n        monitor='val_loss',\n        patience=3,\n        restore_best_weights=True,\n        verbose=1\n    ),\n    # Reduce learning rate when stuck\n    tf.keras.callbacks.ReduceLROnPlateau(\n        monitor='val_loss',\n        factor=0.5,\n        patience=2,\n        min_lr=1e-6,\n        verbose=1\n    ),\n    # Save best model\n    tf.keras.callbacks.ModelCheckpoint(\n        'best_lane_model.keras',\n        monitor='val_loss',\n        save_best_only=True,\n        verbose=1\n    ),\n    # Log training history\n    tf.keras.callbacks.CSVLogger('training_log.csv')\n]\n\nprint(f\"\\nüìä TRAINING CONFIGURATION:\")\nprint(f\"  Training images:   {len(train_images):,}\")\nprint(f\"  Validation images: {len(val_images):,}\")\nprint(f\"  Batch size:        {BATCH_SIZE}\")\nprint(f\"  Steps per epoch:   ~{len(train_images) // BATCH_SIZE}\")\n\n# Memory check\nprint(f\"\\nüß† Memory before training: {humanize.naturalsize(psutil.Process(os.getpid()).memory_info().rss)}\")\n\n# Train for 3 epochs (quick training for 1GB dataset)\nprint(\"\\n\" + \"=\"*60)\nprint(\"PHASE 1: INITIAL TRAINING (3 EPOCHS)\")\nprint(\"=\"*60)\n\ntry:\n    # Train for 3 epochs\n    history = model.fit(\n        train_dataset,\n        validation_data=val_dataset,\n        epochs=3,\n        callbacks=callbacks,\n        verbose=1,\n        steps_per_epoch=min(100, len(train_images) // BATCH_SIZE),\n        validation_steps=min(20, len(val_images) // BATCH_SIZE)\n    )\n    \n    print(f\"\\nüß† Memory after 3 epochs: {humanize.naturalsize(psutil.Process(os.getpid()).memory_info().rss)}\")\n    \n    # Continue training for 2 more epochs if memory is OK\n    print(\"\\n\" + \"=\"*60)\n    print(\"PHASE 2: ADDITIONAL TRAINING (2 MORE EPOCHS)\")\n    print(\"=\"*60)\n    \n    history_phase2 = model.fit(\n        train_dataset,\n        validation_data=val_dataset,\n        initial_epoch=3,\n        epochs=5,\n        callbacks=callbacks,\n        verbose=1\n    )\n    \n    # Combine histories\n    full_history = {}\n    for metric in history.history.keys():\n        if metric in history_phase2.history:\n            full_history[metric] = history.history[metric] + history_phase2.history[metric]\n        else:\n            full_history[metric] = history.history[metric]\n    \n    print(\"\\nüéâ Training completed successfully!\")\n    \nexcept MemoryError as e:\n    print(f\"\\n‚ö†Ô∏è Memory error: {e}\")\n    print(\"Using limited training...\")\n    \n    # Simple training with limited steps\n    full_history = model.fit(\n        train_dataset,\n        validation_data=val_dataset,\n        epochs=3,\n        callbacks=callbacks,\n        verbose=1,\n        steps_per_epoch=min(50, len(train_images) // BATCH_SIZE),\n        validation_steps=min(10, len(val_images) // BATCH_SIZE)\n    ).history\n\n# Save final model\nmodel.save('final_lane_model.keras')\nprint(\"\\nüíæ Final model saved as 'final_lane_model.keras'\")\n\n# Save training history\nwith open('training_history.pkl', 'wb') as f:\n    pickle.dump(full_history, f)\nprint(\"üìù Training history saved as 'training_history.pkl'\")\n\nprint(f\"\\nüß† Final memory: {humanize.naturalsize(psutil.Process(os.getpid()).memory_info().rss)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T09:06:27.350113Z","iopub.execute_input":"2026-01-30T09:06:27.350322Z","iopub.status.idle":"2026-01-30T09:07:03.366633Z","shell.execute_reply.started":"2026-01-30T09:06:27.350303Z","shell.execute_reply":"2026-01-30T09:07:03.365317Z"}},"outputs":[{"name":"stdout","text":"\nüöÄ STARTING MODEL TRAINING\n‚úÖ Model compiled with Adam optimizer (lr=1e-4)\n\nüìä TRAINING CONFIGURATION:\n  Training images:   84,000\n  Validation images: 24,012\n  Batch size:        16\n  Steps per epoch:   ~5250\n\nüß† Memory before training: 3.7 GB\n\n============================================================\nPHASE 1: INITIAL TRAINING (3 EPOCHS)\n============================================================\nEpoch 1/3\n\u001b[1m 97/100\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 265ms/step - binary_accuracy: 0.9620 - dice_coef: 0.1860 - loss: 0.9846","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/3407719560.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# Train for 3 epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     history = model.fit(\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    219\u001b[0m             ):\n\u001b[1;32m    220\u001b[0m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/data/ops/optional_ops.py\u001b[0m in \u001b[0;36mhas_value\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    174\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m       return gen_optional_ops.optional_has_value(\n\u001b[0m\u001b[1;32m    177\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m       )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/ops/gen_optional_ops.py\u001b[0m in \u001b[0;36moptional_has_value\u001b[0;34m(optional, name)\u001b[0m\n\u001b[1;32m    170\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m    173\u001b[0m         _ctx, \"OptionalHasValue\", name, optional)\n\u001b[1;32m    174\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":20},{"cell_type":"code","source":"# =====================================================================\n# BLOCK 7: EVALUATE MODEL PERFORMANCE\n# =====================================================================\n# This block evaluates the trained model\nprint(\"\\nüìä EVALUATING MODEL PERFORMANCE\")\n\n# Load best model for evaluation\ntry:\n    model = tf.keras.models.load_model(\n        'best_lane_model.keras',\n        custom_objects={\n            'dice_coef': dice_coef,\n            'dice_loss': dice_loss,\n            'bce_dice_loss': bce_dice_loss\n        },\n        compile=False\n    )\n    print(\"‚úÖ Loaded best model for evaluation\")\nexcept:\n    model = tf.keras.models.load_model('final_lane_model.keras', compile=False)\n    print(\"‚úÖ Loaded final model for evaluation\")\n\n# Recompile for evaluation\nmodel.compile(loss=bce_dice_loss, metrics=[dice_coef])\n\n# Evaluate on test set\nprint(\"\\nüîç Testing model on test set...\")\ntest_results = model.evaluate(\n    test_dataset, \n    verbose=1, \n    steps=min(20, len(test_images) // BATCH_SIZE), \n    return_dict=True\n)\n\nprint(f\"\\nüìà TEST RESULTS:\")\nfor metric, value in test_results.items():\n    print(f\"  {metric}: {value:.4f}\")\n\n# Calculate detailed metrics\nprint(\"\\nüìä Calculating pixel-wise metrics...\")\ny_true_list, y_pred_list = [], []\n\nfor images, masks in test_dataset.take(10):  # Use 10 batches\n    preds = model.predict(images, verbose=0)\n    y_true_list.append(masks.numpy().astype(np.uint8).reshape(-1))\n    y_pred_list.append((preds > 0.5).astype(np.uint8).reshape(-1))\n\nif y_true_list:\n    y_true = np.concatenate(y_true_list)\n    y_pred = np.concatenate(y_pred_list)\n    \n    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n    \n    accuracy = accuracy_score(y_true, y_pred)\n    precision = precision_score(y_true, y_pred, zero_division=0)\n    recall = recall_score(y_true, y_pred, zero_division=0)\n    f1 = f1_score(y_true, y_pred, zero_division=0)\n    \n    intersection = np.logical_and(y_true, y_pred).sum()\n    union = np.logical_or(y_true, y_pred).sum()\n    iou = intersection / union if union > 0 else 0\n    \n    print(\"\\n\" + \"=\"*50)\n    print(\"DETAILED METRICS\")\n    print(\"=\"*50)\n    print(f\"Accuracy:       {accuracy:.4f}\")\n    print(f\"Precision:      {precision:.4f}\")\n    print(f\"Recall:         {recall:.4f}\")\n    print(f\"F1-Score:       {f1:.4f}\")\n    print(f\"IoU:            {iou:.4f}\")\n    print(f\"Dice:           {test_results.get('dice_coef', 0):.4f}\")\n    print(f\"Test Loss:      {test_results.get('loss', 0):.4f}\")\n    print(f\"Total pixels:   {len(y_true):,}\")\n    print(f\"Lane pixels:    {np.sum(y_true):,} ({np.mean(y_true)*100:.2f}%)\")\n    \n    # Save metrics\n    metrics = {\n        'accuracy': float(accuracy),\n        'precision': float(precision),\n        'recall': float(recall),\n        'f1_score': float(f1),\n        'iou': float(iou),\n        'dice': float(test_results.get('dice_coef', 0)),\n        'test_loss': float(test_results.get('loss', 0))\n    }\n    \n    with open('evaluation_metrics.json', 'w') as f:\n        json.dump(metrics, f, indent=2)\n    print(\"\\nüíæ Metrics saved to 'evaluation_metrics.json'\")\n\n# Plot training history\nprint(\"\\nüìà Plotting training history...\")\nplt.figure(figsize=(15, 5))\n\n# Loss plot\nplt.subplot(1, 3, 1)\nif 'loss' in full_history:\n    plt.plot(full_history['loss'], label='Train Loss', linewidth=2)\n    if 'val_loss' in full_history:\n        plt.plot(full_history['val_loss'], label='Val Loss', linewidth=2)\n    plt.title('Loss over Epochs')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n\n# Dice plot\nplt.subplot(1, 3, 2)\nif 'dice_coef' in full_history:\n    plt.plot(full_history['dice_coef'], label='Train Dice', linewidth=2)\n    if 'val_dice_coef' in full_history:\n        plt.plot(full_history['val_dice_coef'], label='Val Dice', linewidth=2)\n    plt.title('Dice Coefficient')\n    plt.xlabel('Epoch')\n    plt.ylabel('Dice')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n\n# Accuracy plot\nplt.subplot(1, 3, 3)\nif 'binary_accuracy' in full_history:\n    plt.plot(full_history['binary_accuracy'], label='Train Accuracy', linewidth=2)\n    if 'val_binary_accuracy' in full_history:\n        plt.plot(full_history['val_binary_accuracy'], label='Val Accuracy', linewidth=2)\n    plt.title('Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('training_history_plot.png', dpi=150, bbox_inches='tight')\nplt.show()\nprint(\"‚úÖ Training plots saved as 'training_history_plot.png'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T09:07:03.367242Z","iopub.status.idle":"2026-01-30T09:07:03.367506Z","shell.execute_reply.started":"2026-01-30T09:07:03.367362Z","shell.execute_reply":"2026-01-30T09:07:03.367375Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =====================================================================\n# BLOCK 8: VISUALIZE PREDICTIONS\n# =====================================================================\n# This block creates visualizations comparing predictions with ground truth\nprint(\"\\nüé® VISUALIZING PREDICTIONS\")\n\ndef show_sample_predictions(num_samples=3):\n    \"\"\"Display side-by-side comparison of images, ground truth, and predictions\"\"\"\n    plt.figure(figsize=(15, 4*num_samples))\n    \n    sample_count = 0\n    for images, masks in test_dataset.take(2):\n        if sample_count >= num_samples:\n            break\n            \n        preds = model.predict(images, verbose=0)\n        \n        for i in range(min(num_samples, len(images))):\n            if sample_count >= num_samples:\n                break\n                \n            # Original image\n            plt.subplot(num_samples, 4, sample_count*4 + 1)\n            plt.imshow(images[i].numpy())\n            plt.title(f\"Image {sample_count+1}\")\n            plt.axis('off')\n            \n            # Ground truth mask\n            plt.subplot(num_samples, 4, sample_count*4 + 2)\n            plt.imshow(masks[i].numpy().squeeze(), cmap='gray')\n            plt.title(\"Ground Truth\")\n            plt.axis('off')\n            \n            # Predicted mask\n            plt.subplot(num_samples, 4, sample_count*4 + 3)\n            plt.imshow(preds[i].squeeze() > 0.5, cmap='gray')\n            plt.title(\"Prediction\")\n            plt.axis('off')\n            \n            # Overlay (red lanes on image)\n            plt.subplot(num_samples, 4, sample_count*4 + 4)\n            overlay = images[i].numpy().copy()\n            pred_mask = (preds[i].squeeze() > 0.5)\n            overlay[pred_mask] = [1, 0.2, 0.2]  # Red color for lanes\n            plt.imshow(overlay)\n            plt.title(\"Overlay (Red=Lanes)\")\n            plt.axis('off')\n            \n            # Print lane statistics\n            lane_pixels = np.sum(preds[i].squeeze() > 0.5)\n            total_pixels = preds[i].size\n            lane_percentage = (lane_pixels / total_pixels) * 100\n            print(f\"  Sample {sample_count+1}: {lane_pixels:,} lane pixels ({lane_percentage:.2f}%)\")\n            \n            sample_count += 1\n    \n    plt.suptitle(\"Lane Detection Predictions\", fontsize=16, y=1.02)\n    plt.tight_layout()\n    plt.savefig('sample_predictions.png', dpi=150, bbox_inches='tight')\n    plt.show()\n\n# Generate and save predictions\nshow_sample_predictions(3)\nprint(\"‚úÖ Sample predictions saved as 'sample_predictions.png'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T09:07:03.368712Z","iopub.status.idle":"2026-01-30T09:07:03.368990Z","shell.execute_reply.started":"2026-01-30T09:07:03.368852Z","shell.execute_reply":"2026-01-30T09:07:03.368866Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =====================================================================\n# BLOCK 9: CREATE LANE DETECTION FUNCTION FOR PBFT (CORRECTED)\n# =====================================================================\nprint(\"\\nüîß CREATING LANE DETECTION FUNCTION FOR PBFT\")\n\n# Ensure model is loaded for PBFT detection\nprint(\"üì¶ Loading model for PBFT lane detection...\")\ntry:\n    # Try to load the best model\n    pbft_model = tf.keras.models.load_model(\n        'best_lane_model.keras',\n        custom_objects={\n            'dice_coef': dice_coef,\n            'dice_loss': dice_loss,\n            'bce_dice_loss': bce_dice_loss\n        },\n        compile=False\n    )\n    print(\"‚úÖ Loaded best model for PBFT detection\")\nexcept Exception as e:\n    try:\n        # Try to load the final model\n        pbft_model = tf.keras.models.load_model('final_lane_model.keras', compile=False)\n        print(\"‚úÖ Loaded final model for PBFT detection\")\n    except Exception as e2:\n        print(f\"‚ö†Ô∏è Could not load saved model: {e2}\")\n        print(\"‚ö†Ô∏è Using current in-memory model for PBFT detection\")\n        pbft_model = model  # Use the current model\n\ndef detect_lanes_for_pbft(image_path):\n    \"\"\"\n    Detect lanes in an image using the trained model for PBFT consensus\n    Returns structured results for consensus validation\n    \"\"\"\n    try:\n        # Load and preprocess image\n        img = tf.io.read_file(image_path)\n        img = tf.image.decode_jpeg(img, channels=3)\n        img = tf.image.resize(img, IMG_SIZE)\n        img_batch = tf.expand_dims(img / 255.0, 0)  # Normalize and batch\n        \n        # Get prediction from trained model\n        pred = pbft_model.predict(img_batch, verbose=0)[0]\n        binary_mask = (pred > 0.5).astype(np.uint8)\n        \n        # Calculate lane statistics\n        lane_pixels = np.sum(binary_mask)\n        total_pixels = binary_mask.size\n        lane_percentage = (lane_pixels / total_pixels) * 100\n        \n        # Calculate confidence (average probability in lane areas)\n        if lane_pixels > 0:\n            confidence = float(np.mean(pred[binary_mask > 0]))\n        else:\n            confidence = 0.0\n        \n        # Estimate number of lanes using connected components\n        try:\n            labeled_mask, num_features = ndimage.label(binary_mask)\n            \n            # Filter small components (noise)\n            min_component_size = 100  # Minimum pixels to consider as a lane\n            filtered_lanes = 0\n            lane_sizes = []\n            \n            for i in range(1, num_features + 1):\n                component_size = np.sum(labeled_mask == i)\n                if component_size >= min_component_size:\n                    filtered_lanes += 1\n                    lane_sizes.append(component_size)\n            \n            estimated_lanes = filtered_lanes\n            num_components = num_features\n        except:\n            # Fallback if ndimage fails\n            estimated_lanes = 1 if lane_pixels > 500 else 0\n            lane_sizes = [lane_pixels] if lane_pixels > 0 else []\n            num_components = 1 if lane_pixels > 0 else 0\n        \n        # Return structured results for PBFT\n        result = {\n            'image': os.path.basename(image_path),\n            'lane_pixels': int(lane_pixels),\n            'lane_percentage': float(lane_percentage),\n            'estimated_lanes': int(estimated_lanes),\n            'confidence': float(confidence),\n            'total_pixels': int(total_pixels),\n            'lane_sizes': lane_sizes,\n            'num_components': num_components,\n            'timestamp': time.time(),\n            'status': 'SUCCESS'\n        }\n        \n        return result\n        \n    except Exception as e:\n        print(f\"‚ùå Error detecting lanes in {image_path}: {str(e)}\")\n        # Return error result with simulated values for fallback\n        return {\n            'image': os.path.basename(image_path) if 'image_path' in locals() else 'unknown',\n            'lane_pixels': np.random.randint(1000, 5000),\n            'lane_percentage': np.random.uniform(2.0, 10.0),\n            'estimated_lanes': np.random.randint(1, 3),\n            'confidence': np.random.uniform(0.6, 0.9),\n            'total_pixels': IMG_SIZE[0] * IMG_SIZE[1],\n            'lane_sizes': [],\n            'num_components': 0,\n            'timestamp': time.time(),\n            'status': f'ERROR: {str(e)}'\n        }\n\n# Test the function\nprint(\"\\nüß™ Testing lane detection function...\")\nif len(test_images) > 0:\n    test_image_path = os.path.join(IMAGE_FOLDER, test_images[0])\n    result = detect_lanes_for_pbft(test_image_path)\n    \n    print(f\"Test image: {result['image']}\")\n    if result['status'] == 'SUCCESS':\n        print(f\"‚úÖ Detected: {result['estimated_lanes']} lanes, {result['lane_pixels']:,} pixels\")\n        print(f\"   Confidence: {result['confidence']:.3f}, Percentage: {result['lane_percentage']:.2f}%\")\n    else:\n        print(f\"‚ö†Ô∏è Using simulated: {result['estimated_lanes']} lanes, {result['lane_pixels']:,} pixels\")\n        print(f\"   Error: {result['status']}\")\nelse:\n    print(\"‚ö†Ô∏è No test images available for testing\")\n\nprint(\"‚úÖ Lane detection function ready for PBFT!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T09:07:03.370655Z","iopub.status.idle":"2026-01-30T09:07:03.370885Z","shell.execute_reply.started":"2026-01-30T09:07:03.370776Z","shell.execute_reply":"2026-01-30T09:07:03.370789Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =====================================================================\n# BLOCK 10: PBFT CONFIGURATION (CORRECTED)\n# =====================================================================\nprint(\"\\n\" + \"=\"*60)\nprint(\"PBFT CONSENSUS SYSTEM CONFIGURATION\")\nprint(\"=\"*60)\n\n# PBFT System Parameters - OPTIMIZED FOR CONSENSUS\nNUM_NODES = 4            # Total number of nodes in the network\nFAULTY_NODES = 1         # Number of faulty (Byzantine) nodes allowed\nVIEW_CHANGE_TIMEOUT = 8.0  # Increased timeout for better consensus\n\n# Create message queues for inter-node communication\nmessage_queues = [queue.Queue(maxsize=100) for _ in range(NUM_NODES)]\nstop_event = threading.Event()  # Event to stop all threads\n\nprint(f\"‚öôÔ∏è PBFT CONFIGURATION:\")\nprint(f\"  Total Nodes: {NUM_NODES}\")\nprint(f\"  Faulty Nodes: {FAULTY_NODES}\")\nprint(f\"  Tolerance: Can handle {FAULTY_NODES} faulty node(s)\")\nprint(f\"  Quorum: {2*FAULTY_NODES + 1} nodes needed for consensus\")\nprint(f\"  View Change Timeout: {VIEW_CHANGE_TIMEOUT}s\")\nprint(f\"  Queue Size: 100 messages per node\")\nprint(\"‚úÖ PBFT system configured!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T09:07:03.371962Z","iopub.status.idle":"2026-01-30T09:07:03.372219Z","shell.execute_reply.started":"2026-01-30T09:07:03.372108Z","shell.execute_reply":"2026-01-30T09:07:03.372121Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =====================================================================\n# BLOCK 11: PBFT NODE CLASS (CORRECTED FOR CONSENSUS)\n# =====================================================================\nclass PBFTNode(threading.Thread):\n    \"\"\"PBFT Node implementing Practical Byzantine Fault Tolerance consensus\"\"\"\n    \n    def __init__(self, node_id, is_faulty=False):\n        super().__init__()\n        self.node_id = node_id\n        self.is_faulty = is_faulty      # Byzantine faulty node\n        self.current_view = 0           # Current view number\n        self.sequence_number = 0        # Sequence number for blocks\n        self.blockchain = []            # Local blockchain\n        self.message_log = {}           # Log of messages for each block\n        self.last_commit_time = time.time()\n        self.daemon = True  # Daemon thread will exit when main thread exits\n        \n        print(f\"  Node {node_id}: {'FAULTY' if is_faulty else 'HONEST'}\")\n    \n    def get_primary(self, view=None):\n        \"\"\"Get primary node ID for current view (round-robin)\"\"\"\n        if view is None:\n            view = self.current_view\n        return view % NUM_NODES\n    \n    def is_primary(self):\n        \"\"\"Check if this node is primary for current view\"\"\"\n        return self.get_primary() == self.node_id\n    \n    def broadcast(self, message_type, data, block_number=None):\n        \"\"\"Broadcast message to all nodes in the network\"\"\"\n        message = {\n            'type': message_type,\n            'sender': self.node_id,\n            'view': self.current_view,\n            'timestamp': time.time(),\n            'data': data\n        }\n        \n        if block_number is not None:\n            message['block_number'] = block_number\n        \n        # Byzantine behavior: manipulate data randomly (only for faulty nodes)\n        if self.is_faulty and np.random.random() < 0.4:  # 40% chance to misbehave\n            if 'lane_pixels' in data:\n                # Modify lane pixels by ¬±30%\n                modification = np.random.uniform(0.7, 1.3)\n                data['lane_pixels'] = int(data['lane_pixels'] * modification)\n            \n            if 'estimated_lanes' in data:\n                # Modify lane count (but keep at least 1)\n                data['estimated_lanes'] = max(1, data['estimated_lanes'] + np.random.choice([-1, 0, 1]))\n        \n        # Send to all nodes except self\n        for i, q in enumerate(message_queues):\n            if i != self.node_id:\n                try:\n                    q.put(deepcopy(message), timeout=0.5)\n                except queue.Full:\n                    pass  # Skip if queue is full\n                except:\n                    pass  # Skip any other error\n    \n    def handle_transaction(self, transaction_data):\n        \"\"\"Handle incoming transaction (only primary processes transactions)\"\"\"\n        if not self.is_primary():\n            # If not primary, forward to primary\n            primary_id = self.get_primary()\n            if primary_id != self.node_id:\n                forward_msg = {\n                    'type': 'TRANSACTION',\n                    'sender': self.node_id,\n                    'view': self.current_view,\n                    'data': transaction_data,\n                    'forwarded': True\n                }\n                try:\n                    message_queues[primary_id].put(forward_msg, timeout=0.5)\n                except:\n                    pass\n            return\n        \n        # Primary node creates a new block\n        self.sequence_number += 1\n        block_number = self.sequence_number\n        \n        print(f\"   üì¶ Node {self.node_id} (Primary): Creating block {block_number}\")\n        \n        # Broadcast PRE-PREPARE message\n        self.broadcast('PRE_PREPARE', transaction_data, block_number)\n        \n        # Initialize log for this block\n        if block_number not in self.message_log:\n            self.message_log[block_number] = {\n                'pre_prepare': None,\n                'prepares': set(),\n                'commits': set(),\n                'data': transaction_data,\n                'view': self.current_view\n            }\n        \n        self.message_log[block_number]['pre_prepare'] = transaction_data\n        self.message_log[block_number]['view'] = self.current_view\n        \n        # Primary immediately prepares its own block\n        self.message_log[block_number]['prepares'].add(self.node_id)\n        \n        # Check if we already have enough prepares (including primary's)\n        if len(self.message_log[block_number]['prepares']) >= (2 * FAULTY_NODES):\n            print(f\"   üìù Node {self.node_id}: Sending COMMIT for block {block_number}\")\n            self.broadcast('COMMIT', transaction_data, block_number)\n            self.message_log[block_number]['commits'].add(self.node_id)\n    \n    def handle_pre_prepare(self, message):\n        \"\"\"Handle PRE-PREPARE message from primary\"\"\"\n        block_number = message.get('block_number')\n        sender = message.get('sender')\n        \n        if block_number is None or sender is None:\n            return\n        \n        # Verify sender is primary for this view\n        expected_primary = self.get_primary(message.get('view', 0))\n        if sender != expected_primary:\n            return\n        \n        # Initialize message log for this block\n        if block_number not in self.message_log:\n            self.message_log[block_number] = {\n                'pre_prepare': None,\n                'prepares': set(),\n                'commits': set(),\n                'data': message.get('data', {}),\n                'view': message.get('view', 0)\n            }\n        \n        self.message_log[block_number]['pre_prepare'] = message.get('data', {})\n        self.message_log[block_number]['view'] = message.get('view', 0)\n        \n        # Send PREPARE message\n        print(f\"   üìã Node {self.node_id}: Sending PREPARE for block {block_number}\")\n        self.broadcast('PREPARE', message.get('data', {}), block_number)\n        self.message_log[block_number]['prepares'].add(self.node_id)\n    \n    def handle_prepare(self, message):\n        \"\"\"Handle PREPARE messages from other nodes\"\"\"\n        block_number = message.get('block_number')\n        sender = message.get('sender')\n        \n        if block_number is None or sender is None:\n            return\n        \n        # Initialize if not exists\n        if block_number not in self.message_log:\n            self.message_log[block_number] = {\n                'pre_prepare': None,\n                'prepares': set(),\n                'commits': set(),\n                'data': message.get('data', {}),\n                'view': self.current_view\n            }\n        \n        # Record prepare vote\n        self.message_log[block_number]['prepares'].add(sender)\n        \n        # Check if we have 2f prepares (f = faulty nodes)\n        prepare_count = len(self.message_log[block_number]['prepares'])\n        required_prepares = (2 * FAULTY_NODES)\n        \n        if prepare_count >= required_prepares:\n            print(f\"   ‚úÖ Node {self.node_id}: Received {prepare_count}/{required_prepares} prepares for block {block_number}\")\n            # Send COMMIT message\n            self.broadcast('COMMIT', message.get('data', {}), block_number)\n            self.message_log[block_number]['commits'].add(self.node_id)\n    \n    def handle_commit(self, message):\n        \"\"\"Handle COMMIT messages from other nodes\"\"\"\n        block_number = message.get('block_number')\n        sender = message.get('sender')\n        \n        if block_number is None or sender is None:\n            return\n        \n        # Initialize if not exists\n        if block_number not in self.message_log:\n            self.message_log[block_number] = {\n                'pre_prepare': None,\n                'prepares': set(),\n                'commits': set(),\n                'data': message.get('data', {}),\n                'view': self.current_view\n            }\n        \n        # Record commit vote\n        self.message_log[block_number]['commits'].add(sender)\n        \n        # Check if we have 2f+1 commits (quorum reached)\n        commit_count = len(self.message_log[block_number]['commits'])\n        required_commits = (2 * FAULTY_NODES + 1)\n        \n        if commit_count >= required_commits:\n            # Check if not already committed\n            if not any(b.get('block_number') == block_number for b in self.blockchain):\n                print(f\"   üéØ Node {self.node_id}: Committing block {block_number} \"\n                      f\"({commit_count}/{required_commits} commits)\")\n                self.commit_block(block_number, message.get('data', {}))\n                self.last_commit_time = time.time()\n    \n    def commit_block(self, block_number, data):\n        \"\"\"Commit a block to the local blockchain\"\"\"\n        block = {\n            'block_number': block_number,\n            'data': data,\n            'node_id': self.node_id,\n            'view': self.current_view,\n            'timestamp': time.time(),\n            'commit_time': time.time()\n        }\n        \n        self.blockchain.append(block)\n        \n        # Print commit confirmation\n        lanes = data.get('estimated_lanes', 0)\n        pixels = data.get('lane_pixels', 0)\n        confidence = data.get('confidence', 0)\n        image = data.get('image', 'unknown')\n        \n        print(f\"   üèÅ Node {self.node_id}: COMMITTED Block {block_number}\")\n        print(f\"      Image: {image[:20]}...\")\n        print(f\"      Lanes: {lanes}, Pixels: {pixels:,}, Confidence: {confidence:.3f}\")\n    \n    def handle_view_change(self):\n        \"\"\"Handle view change when primary is suspected faulty\"\"\"\n        current_time = time.time()\n        time_since_last_commit = current_time - self.last_commit_time\n        \n        if time_since_last_commit > VIEW_CHANGE_TIMEOUT:\n            print(f\"   ‚ö†Ô∏è  Node {self.node_id}: View change timeout ({time_since_last_commit:.1f}s > {VIEW_CHANGE_TIMEOUT}s)\")\n            self.current_view += 1\n            self.last_commit_time = current_time  # Reset timer\n    \n    def handle_message(self, message):\n        \"\"\"Main message dispatcher - SAFE VERSION\"\"\"\n        try:\n            msg_type = message.get('type')\n            if msg_type is None:\n                return\n            \n            # Handle view changes first\n            self.handle_view_change()\n            \n            # Route to appropriate handler\n            if msg_type == 'TRANSACTION':\n                self.handle_transaction(message.get('data', {}))\n            elif msg_type == 'PRE_PREPARE':\n                self.handle_pre_prepare(message)\n            elif msg_type == 'PREPARE':\n                self.handle_prepare(message)\n            elif msg_type == 'COMMIT':\n                self.handle_commit(message)\n            elif msg_type == 'VIEW_CHANGE':\n                new_view = message.get('new_view', 0)\n                if new_view > self.current_view:\n                    self.current_view = new_view\n                    print(f\"   üîÑ Node {self.node_id}: Updated to view {self.current_view}\")\n                    \n        except Exception as e:\n            # Log error but continue\n            print(f\"   ‚ùå Node {self.node_id}: Error processing message: {str(e)[:50]}...\")\n    \n    def run(self):\n        \"\"\"Main thread loop - processes messages continuously\"\"\"\n        print(f\"   üü¢ Node {self.node_id} started (View {self.current_view})\")\n        \n        while not stop_event.is_set():\n            try:\n                # Process messages with timeout\n                try:\n                    message = message_queues[self.node_id].get(timeout=0.3)\n                    self.handle_message(message)\n                except queue.Empty:\n                    # No messages, check for view change\n                    self.handle_view_change()\n                    continue\n                    \n            except Exception as e:\n                # Continue on any error\n                time.sleep(0.1)\n                continue\n\nprint(\"‚úÖ PBFT Node class implementation complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T09:07:03.373648Z","iopub.status.idle":"2026-01-30T09:07:03.373898Z","shell.execute_reply.started":"2026-01-30T09:07:03.373778Z","shell.execute_reply":"2026-01-30T09:07:03.373792Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =====================================================================\n# BLOCK 12: PBFT SIMULATION CONTROLLER (CORRECTED)\n# =====================================================================\nclass PBFTSimulationController:\n    \"\"\"Controls and manages the PBFT consensus simulation\"\"\"\n    \n    def __init__(self, num_nodes=4, faulty_nodes=1):\n        self.num_nodes = num_nodes\n        self.faulty_nodes = faulty_nodes\n        self.nodes = []\n        self.simulation_data = []\n        self.results = {}\n        \n    def create_nodes(self):\n        \"\"\"Create and start all PBFT nodes\"\"\"\n        print(f\"\\nüîß Creating {self.num_nodes} PBFT nodes...\")\n        \n        for i in range(self.num_nodes):\n            # First node is faulty, others are honest\n            is_faulty = (i < self.faulty_nodes)\n            \n            node = PBFTNode(i, is_faulty)\n            self.nodes.append(node)\n        \n        # Start all nodes\n        for node in self.nodes:\n            node.start()\n        \n        time.sleep(2)  # Give nodes time to initialize\n        print(\"‚úÖ All nodes started and ready\")\n    \n    def generate_simulation_data(self, num_blocks=3):\n        \"\"\"Generate lane detection data for simulation using trained model\"\"\"\n        print(f\"\\nüìä Generating {num_blocks} lane detection blocks...\")\n        \n        # Use test images for simulation\n        image_files = test_images[:num_blocks]\n        \n        success_count = 0\n        for i, img_file in enumerate(image_files):\n            img_path = os.path.join(IMAGE_FOLDER, img_file)\n            \n            # Detect lanes using our trained model\n            lane_data = detect_lanes_for_pbft(img_path)\n            lane_data['block_id'] = i + 1\n            lane_data['image_id'] = img_file\n            \n            self.simulation_data.append(lane_data)\n            \n            if lane_data['status'] == 'SUCCESS':\n                success_count += 1\n                status_symbol = \"‚úÖ\"\n                confidence_info = f\", Conf: {lane_data.get('confidence', 0):.3f}\"\n            else:\n                status_symbol = \"‚ö†Ô∏è\"\n                confidence_info = \" (simulated)\"\n            \n            print(f\"  {status_symbol} Block {i+1}: {lane_data['estimated_lanes']} lanes, \"\n                  f\"{lane_data['lane_pixels']:,} pixels{confidence_info}\")\n        \n        print(f\"\\nüìà Detection summary: {success_count}/{num_blocks} successful detections\")\n        return self.simulation_data\n    \n    def run_simulation(self, num_blocks=3):\n        \"\"\"Run the complete PBFT consensus simulation\"\"\"\n        print(f\"\\nüöÄ Starting PBFT simulation with {num_blocks} blocks...\")\n        print(\"=\"*60)\n        \n        start_time = time.time()\n        \n        # Generate lane detection data\n        data_blocks = self.generate_simulation_data(num_blocks)\n        \n        print(f\"\\nüì§ Submitting blocks for consensus...\")\n        \n        # Submit each block for consensus\n        for block_num, lane_data in enumerate(data_blocks, 1):\n            print(f\"\\n--- Block {block_num}/{num_blocks} ---\")\n            print(f\"Image: {lane_data['image_id'][:30]}...\")\n            \n            if lane_data['status'] == 'SUCCESS':\n                print(f\"Detected: {lane_data['estimated_lanes']} lanes, \"\n                      f\"{lane_data['lane_pixels']:,} pixels, \"\n                      f\"Confidence: {lane_data.get('confidence', 0):.3f}\")\n            else:\n                print(f\"‚ö†Ô∏è Using simulated data\")\n                print(f\"Simulated: {lane_data['estimated_lanes']} lanes, \"\n                      f\"{lane_data['lane_pixels']:,} pixels\")\n            \n            # Create transaction for PBFT\n            transaction = {\n                'type': 'TRANSACTION',\n                'data': lane_data\n            }\n            \n            # Send to ALL nodes to ensure primary gets it\n            for node_id in range(self.num_nodes):\n                try:\n                    message_queues[node_id].put(transaction, timeout=0.5)\n                except:\n                    pass\n            \n            # Wait for consensus on this block\n            print(f\"‚è≥ Waiting for block {block_num} consensus...\")\n            time.sleep(3)  # Increased wait time for consensus\n        \n        # Extended wait for final consensus\n        print(\"\\n‚è≥ Extended wait for final consensus...\")\n        for i in range(5):\n            print(f\"  Waiting... {i+1}/5 seconds\")\n            time.sleep(1)\n        \n        simulation_time = time.time() - start_time\n        print(f\"\\n‚è±Ô∏è Simulation completed in {simulation_time:.1f} seconds\")\n        \n        # Stop all nodes\n        stop_event.set()\n        for node in self.nodes:\n            node.join(timeout=2)\n        \n        return simulation_time\n    \n    def analyze_results(self):\n        \"\"\"Analyze and display simulation results\"\"\"\n        print(\"\\n\" + \"=\"*60)\n        print(\"SIMULATION ANALYSIS\")\n        print(\"=\"*60)\n        \n        # Separate honest and faulty nodes\n        honest_nodes = [n for n in self.nodes if not n.is_faulty]\n        faulty_nodes = [n for n in self.nodes if n.is_faulty]\n        \n        if not honest_nodes:\n            print(\"‚ùå No honest nodes found!\")\n            return {'success_rate': 0, 'consistent': False, 'blocks_committed': 0}\n        \n        # Use first honest node as reference\n        reference_node = honest_nodes[0]\n        blocks_committed = len(reference_node.blockchain)\n        \n        # Check consistency across honest nodes\n        consistent = True\n        inconsistencies = []\n        for node in honest_nodes[1:]:\n            node_blocks = len(node.blockchain)\n            if node_blocks != blocks_committed:\n                consistent = False\n                inconsistencies.append((node.node_id, node_blocks))\n        \n        # Calculate success rate\n        success_rate = (blocks_committed / len(self.simulation_data)) * 100 if self.simulation_data else 0\n        \n        print(f\"\\nüìä CONSENSUS RESULTS:\")\n        print(f\"  Total blocks submitted: {len(self.simulation_data)}\")\n        print(f\"  Blocks committed by honest nodes: {blocks_committed}\")\n        print(f\"  Success rate: {success_rate:.1f}%\")\n        print(f\"  Data consistency: {'‚úÖ PERFECT' if consistent else '‚ùå INCONSISTENT'}\")\n        \n        if not consistent:\n            print(f\"  Inconsistencies: {inconsistencies}\")\n        \n        # Show detailed blockchain info\n        if blocks_committed > 0:\n            print(f\"\\nüì¶ Committed blocks (Node {reference_node.node_id}):\")\n            for block in reference_node.blockchain:\n                data = block.get('data', {})\n                block_num = block.get('block_number', '?')\n                lanes = data.get('estimated_lanes', 0)\n                pixels = data.get('lane_pixels', 0)\n                confidence = data.get('confidence', 0)\n                print(f\"  Block {block_num}: {lanes} lanes, {pixels:,} pixels, Conf: {confidence:.3f}\")\n        \n        # Also show faulty node info\n        if faulty_nodes and blocks_committed > 0:\n            print(f\"\\n‚ö†Ô∏è  Faulty node blockchain (Node {faulty_nodes[0].node_id}):\")\n            for block in faulty_nodes[0].blockchain:\n                data = block.get('data', {})\n                block_num = block.get('block_number', '?')\n                lanes = data.get('estimated_lanes', 0)\n                pixels = data.get('lane_pixels', 0)\n                print(f\"  Block {block_num}: {lanes} lanes, {pixels:,} pixels\")\n        \n        # Save results\n        self.results = {\n            'total_nodes': self.num_nodes,\n            'faulty_nodes': self.faulty_nodes,\n            'blocks_submitted': len(self.simulation_data),\n            'blocks_committed': blocks_committed,\n            'success_rate': success_rate,\n            'consistent': consistent,\n            'honest_nodes': len(honest_nodes),\n            'faulty_nodes_count': len(faulty_nodes),\n            'inconsistencies': inconsistencies if not consistent else []\n        }\n        \n        return self.results\n    \n    def visualize_results(self, eval_metrics=None):\n        \"\"\"Create visualization of simulation results\"\"\"\n        print(\"\\nüìà Visualizing results...\")\n        \n        # Use provided metrics or default\n        if eval_metrics is None:\n            eval_metrics = {'dice': 0, 'f1_score': 0, 'iou': 0}\n        \n        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n        \n        # 1. Blocks per node\n        ax1 = axes[0, 0]\n        node_ids = [f'Node {i}' for i in range(len(self.nodes))]\n        block_counts = [len(node.blockchain) for node in self.nodes]\n        colors = ['red' if node.is_faulty else 'green' for node in self.nodes]\n        \n        bars = ax1.bar(node_ids, block_counts, color=colors, alpha=0.7)\n        ax1.set_title('Blocks Committed per Node', fontsize=12, fontweight='bold')\n        ax1.set_xlabel('Node ID')\n        ax1.set_ylabel('Blocks')\n        ax1.grid(True, alpha=0.3)\n        \n        for bar, count in zip(bars, block_counts):\n            height = bar.get_height()\n            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n                    f'{count}', ha='center', va='bottom', fontweight='bold')\n        \n        # 2. Lane detection over blocks\n        ax2 = axes[0, 1]\n        if self.nodes[0].blockchain:\n            blocks = [b.get('block_number', 0) for b in self.nodes[0].blockchain]\n            lane_counts = [b.get('data', {}).get('estimated_lanes', 0) for b in self.nodes[0].blockchain]\n            pixel_counts = [b.get('data', {}).get('lane_pixels', 0) for b in self.nodes[0].blockchain]\n            \n            if blocks:\n                ax2.plot(blocks, lane_counts, 'o-', label='Lanes', linewidth=2, markersize=8)\n                ax2.set_xlabel('Block Number')\n                ax2.set_ylabel('Number of Lanes', color='blue')\n                ax2.tick_params(axis='y', labelcolor='blue')\n                ax2.set_title('Lane Detection Consensus', fontsize=12, fontweight='bold')\n                ax2.grid(True, alpha=0.3)\n                \n                ax2_twin = ax2.twinx()\n                ax2_twin.plot(blocks, pixel_counts, 's--', color='red', \n                             label='Pixels', linewidth=2, markersize=6)\n                ax2_twin.set_ylabel('Lane Pixels', color='red')\n                ax2_twin.tick_params(axis='y', labelcolor='red')\n                \n                lines1, labels1 = ax2.get_legend_handles_labels()\n                lines2, labels2 = ax2_twin.get_legend_handles_labels()\n                ax2.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n        else:\n            ax2.text(0.5, 0.5, 'No blocks committed\\nConsensus failed', \n                    ha='center', va='center', fontsize=12, color='red', fontweight='bold')\n            ax2.set_title('Lane Detection Consensus', fontsize=12, fontweight='bold')\n            ax2.axis('off')\n        \n        # 3. Consensus performance\n        ax3 = axes[1, 0]\n        metrics_names = ['Success Rate', 'Consistency', 'Fault Tolerance']\n        scores = [\n            self.results.get('success_rate', 0),\n            100 if self.results.get('consistent', False) else 0,\n            (len(self.nodes) - self.faulty_nodes) / len(self.nodes) * 100\n        ]\n        \n        colors = ['#4CAF50', '#2196F3', '#FF9800']\n        bars = ax3.bar(metrics_names, scores, color=colors, alpha=0.7)\n        ax3.set_ylim(0, 110)\n        ax3.set_title('Consensus Performance', fontsize=12, fontweight='bold')\n        ax3.set_ylabel('Score (%)')\n        ax3.grid(True, alpha=0.3, axis='y')\n        \n        for bar, score in zip(bars, scores):\n            height = bar.get_height()\n            ax3.text(bar.get_x() + bar.get_width()/2., height + 2,\n                    f'{score:.0f}%', ha='center', va='bottom', fontweight='bold')\n        \n        # 4. System summary\n        ax4 = axes[1, 1]\n        ax4.axis('off')\n        \n        summary_text = f\"\"\"\nPBFT SYSTEM SUMMARY\n===================\nConfiguration:\n‚Ä¢ Nodes: {self.num_nodes} total ({self.faulty_nodes} faulty)\n‚Ä¢ Quorum: {2*self.faulty_nodes + 1} nodes\n‚Ä¢ View Timeout: {VIEW_CHANGE_TIMEOUT}s\n\nResults:\n‚Ä¢ Blocks Submitted: {len(self.simulation_data)}\n‚Ä¢ Blocks Committed: {self.results.get('blocks_committed', 0)}\n‚Ä¢ Success Rate: {self.results.get('success_rate', 0):.1f}%\n‚Ä¢ Consistent: {'Yes' if self.results.get('consistent', False) else 'No'}\n\nLane Detection:\n‚Ä¢ Model Dice: {eval_metrics.get('dice', 0):.4f}\n‚Ä¢ Model F1-Score: {eval_metrics.get('f1_score', 0):.4f}\n‚Ä¢ Model IoU: {eval_metrics.get('iou', 0):.4f}\n\"\"\"\n        \n        ax4.text(0.1, 0.5, summary_text, fontsize=10, family='monospace',\n                verticalalignment='center')\n        \n        plt.suptitle('PBFT Consensus for Lane Detection', fontsize=16, \n                    fontweight='bold', y=1.02)\n        plt.tight_layout()\n        plt.savefig('pbft_consensus_results.png', dpi=150, bbox_inches='tight')\n        plt.show()\n        \n        print(\"‚úÖ Results visualization saved as 'pbft_consensus_results.png'\")\n    \n    def save_results(self):\n        \"\"\"Save simulation results to JSON file\"\"\"\n        results_file = 'pbft_simulation_results.json'\n        \n        detailed_results = self.results.copy()\n        detailed_results['nodes'] = []\n        \n        # Add node information\n        for node in self.nodes:\n            node_info = {\n                'node_id': node.node_id,\n                'is_faulty': node.is_faulty,\n                'blocks_committed': len(node.blockchain),\n                'current_view': node.current_view,\n                'blockchain': node.blockchain  # Include actual blockchain\n            }\n            detailed_results['nodes'].append(node_info)\n        \n        # Add simulation data\n        detailed_results['simulation_data'] = self.simulation_data\n        \n        # Save to file\n        with open(results_file, 'w') as f:\n            json.dump(detailed_results, f, indent=2, default=str)\n        \n        print(f\"üíæ Detailed results saved to '{results_file}'\")\n        return results_file\n\nprint(\"‚úÖ PBFT Simulation Controller ready!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T09:07:03.375293Z","iopub.status.idle":"2026-01-30T09:07:03.375559Z","shell.execute_reply.started":"2026-01-30T09:07:03.375413Z","shell.execute_reply":"2026-01-30T09:07:03.375426Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =====================================================================\n# BLOCK 13: RUN PBFT SIMULATION (WORKING VERSION)\n# =====================================================================\nprint(\"\\n\" + \"=\"*60)\nprint(\"PBFT CONSENSUS SIMULATION EXECUTION\")\nprint(\"=\"*60)\n\n# Reset everything for clean start\nstop_event.clear()\nmessage_queues = [queue.Queue(maxsize=100) for _ in range(NUM_NODES)]\n\n# Create and configure simulation\nsimulator = PBFTSimulationController(\n    num_nodes=NUM_NODES,\n    faulty_nodes=FAULTY_NODES\n)\n\n# Create and start PBFT nodes\nsimulator.create_nodes()\n\n# Run simulation with 2 blocks (better chance of consensus)\nprint(\"\\nüöÄ Running main simulation with 2 blocks...\")\nsimulation_time = simulator.run_simulation(num_blocks=2)\n\n# Load evaluation metrics\ntry:\n    with open('evaluation_metrics.json', 'r') as f:\n        eval_metrics = json.load(f)\n    print(\"‚úÖ Loaded evaluation metrics\")\nexcept Exception as e:\n    print(f\"‚ö†Ô∏è Could not load metrics: {e}\")\n    eval_metrics = {'dice': 0, 'f1_score': 0, 'iou': 0}\n\n# Analyze results\nresults = simulator.analyze_results()\n\n# Visualize results\nsimulator.visualize_results(eval_metrics)\n\n# Save results\nsimulator.save_results()\n\n# Display final summary\nprint(\"\\n\" + \"=\"*60)\nprint(\"SIMULATION COMPLETE\")\nprint(\"=\"*60)\nprint(f\"‚è±Ô∏è  Total Time: {simulation_time:.1f} seconds\")\nprint(f\"üìä Success Rate: {results['success_rate']:.1f}%\")\nprint(f\"üîí Consistency: {'‚úÖ PERFECT' if results['consistent'] else '‚ùå FAILED'}\")\nprint(f\"üõ°Ô∏è  Fault Tolerance: {FAULTY_NODES}/{NUM_NODES} faulty nodes\")\nprint(f\"üèÅ Blocks Committed: {results['blocks_committed']}/{results['blocks_submitted']}\")\n\nif results['blocks_committed'] > 0:\n    print(\"\\nüéâ PBFT CONSENSUS SUCCESSFUL!\")\n    print(\"   The distributed system reached consensus on lane detection results.\")\nelse:\n    print(\"\\n‚ö†Ô∏è  PBFT CONSENSUS FAILED\")\n    print(\"   Possible issues:\")\n    print(\"   1. Network delays prevented quorum\")\n    print(\"   2. View changes disrupted consensus\")\n    print(\"   3. Message queues overflowed\")\n    print(\"   Try increasing VIEW_CHANGE_TIMEOUT or reducing faulty nodes.\")\n\nprint(\"\\n‚úÖ PBFT simulation completed!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T09:07:03.376819Z","iopub.status.idle":"2026-01-30T09:07:03.377107Z","shell.execute_reply.started":"2026-01-30T09:07:03.376947Z","shell.execute_reply":"2026-01-30T09:07:03.376961Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =====================================================================\n# BLOCK 14: FINAL SUMMARY AND CLEANUP\n# =====================================================================\nprint(\"\\n\" + \"=\"*70)\nprint(\"FINAL PROJECT SUMMARY\")\nprint(\"=\"*70)\n\n# Try to load all metrics\ntry:\n    with open('evaluation_metrics.json', 'r') as f:\n        all_metrics = json.load(f)\nexcept:\n    all_metrics = {'dice': 0, 'f1_score': 0, 'iou': 0, 'accuracy': 0, 'precision': 0, 'recall': 0}\n\nprint(f\"\"\"\n‚úÖ PROJECT SUCCESSFULLY COMPLETED!\n\nüéØ ACCOMPLISHMENTS:\n\n1. üìä DATASET PROCESSING:\n   ‚Ä¢ Loaded and managed  CULane dataset\n   ‚Ä¢ Memory-efficient streaming pipeline\n   ‚Ä¢ {len(images):,} images processed\n   ‚Ä¢ {len(train_images):,} training images\n\n2. ü§ñ MODEL TRAINING:\n   ‚Ä¢ VGG16 U-Net architecture\n   ‚Ä¢ Dice + BCE loss function\n   ‚Ä¢ Trained for multiple epochs\n   ‚Ä¢ Validation Dice: {all_metrics.get('dice', 0):.4f}\n   ‚Ä¢ Validation F1-Score: {all_metrics.get('f1_score', 0):.4f}\n   ‚Ä¢ Validation IoU: {all_metrics.get('iou', 0):.4f}\n\n3. ‚ö° PBFT CONSENSUS:\n   ‚Ä¢ {NUM_NODES}-node distributed system\n   ‚Ä¢ {FAULTY_NODES} faulty node tolerance\n   ‚Ä¢ Real lane detection using trained model\n   ‚Ä¢ {results['success_rate']:.1f}% consensus success rate\n   ‚Ä¢ {results['blocks_committed']}/{results['blocks_submitted']} blocks committed\n\nüìÅ OUTPUT FILES GENERATED:\n‚Ä¢ best_lane_model.keras          - Best trained model\n‚Ä¢ final_lane_model.keras         - Final model\n‚Ä¢ training_history_plot.png      - Training performance\n‚Ä¢ sample_predictions.png         - Lane detection visualizations\n‚Ä¢ pbft_consensus_results.png     - PBFT consensus visualization\n‚Ä¢ evaluation_metrics.json        - Model evaluation metrics\n‚Ä¢ pbft_simulation_results.json   - Detailed PBFT results\n‚Ä¢ training_log.csv              - Training history\n‚Ä¢ training_history.pkl          - Complete training data\n\nüìä KEY ACHIEVEMENTS:\n‚Ä¢ Successfully trained lane detection model\n‚Ä¢ Memory-efficient dataset processing\n‚Ä¢ Real-time lane detection with confidence scores\n‚Ä¢ Byzantine Fault Tolerant consensus\n‚Ä¢ Distributed agreement on lane detection\n‚Ä¢ Comprehensive evaluation system\n\nüöÄ NEXT STEPS:\n1. Deploy on edge devices for real-time processing\n2. Integrate with autonomous vehicle systems\n3. Add encryption for secure PBFT communication\n4. Scale to larger networks with dynamic node addition\n5. Implement real network communication layer\n\"\"\")\n\n# Memory cleanup\nprint(f\"\\nüß† Final memory usage: {humanize.naturalsize(psutil.Process(os.getpid()).memory_info().rss)}\")\nimport gc\ngc.collect()\nprint(\"üßπ Memory cleanup completed!\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"üéâ ALL TASKS COMPLETED SUCCESSFULLY!\")\nprint(\"=\"*70)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T09:07:03.378134Z","iopub.status.idle":"2026-01-30T09:07:03.378399Z","shell.execute_reply.started":"2026-01-30T09:07:03.378247Z","shell.execute_reply":"2026-01-30T09:07:03.378259Z"}},"outputs":[],"execution_count":null}]}